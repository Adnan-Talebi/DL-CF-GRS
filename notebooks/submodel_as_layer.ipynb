{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T06:38:37.046266Z",
     "start_time": "2022-10-11T06:38:34.128347Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/workspace/rs-data-python\")\n",
    "sys.path.append(\".\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from data_utils import init_random\n",
    "init_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T06:38:37.418472Z",
     "start_time": "2022-10-11T06:38:37.048518Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_groups import GroupDataFT\n",
    "from data_groups import OneHotGenerator\n",
    "dataset = GroupDataFT()\n",
    "ds_shape, nuser, nitems, ds_code = dataset.get_shape(), dataset.get_num_users(), dataset.get_num_items(), dataset.get_data_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T06:38:39.345082Z",
     "start_time": "2022-10-11T06:38:37.421147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"preliminar\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "entrada (InputLayer)            [(None, 3579)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1508)         0           entrada[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2071)         0           entrada[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb_u (Dense)                   (None, 8)            12072       lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "emb_i (Dense)                   (None, 8)            16576       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8)            0           emb_u[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8)            0           emb_i[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16)           0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer64 (Dense)                 (None, 64)           1088        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer32 (Dense)                 (None, 32)           2080        layer64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer16 (Dense)                 (None, 16)           528         layer32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer8 (Dense)                  (None, 8)            136         layer16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Dense)              (None, 1)            9           layer8[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 32,489\n",
      "Trainable params: 32,489\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Reshape, Flatten, Concatenate, Multiply\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k=8\n",
    "seed=37\n",
    "model_name='preliminar'\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "regs=[0,0]\n",
    "    \n",
    "input_layer = layers.Input(shape=ds_shape, name=\"entrada\")\n",
    "\n",
    "e_user = layers.Dense(\n",
    "    k,\n",
    "    name=\"emb_u\",\n",
    "    kernel_initializer = 'normal', kernel_regularizer = l2(regs[0])\n",
    ")(layers.Lambda(lambda x: x[:, 0:nuser])(input_layer))\n",
    "\n",
    "e_item = layers.Dense(\n",
    "    k,\n",
    "    name=\"emb_i\",\n",
    "    kernel_initializer = 'normal', kernel_regularizer = l2(regs[1])\n",
    ")(layers.Lambda(lambda x: x[:, nuser:])(input_layer))\n",
    "\n",
    "# Crucial to flatten an embedding vector!\n",
    "user_latent = Flatten()(e_user)\n",
    "item_latent = Flatten()(e_item)\n",
    "\n",
    "# Element-wise product of user and item embeddings \n",
    "#predict_vector = merge([user_latent, item_latent], mode = 'mul')\n",
    "vector = Concatenate()([user_latent, item_latent])\n",
    "# MLP layers\n",
    "for idx in [64,32,16,8]:\n",
    "    layer = Dense(idx, kernel_regularizer= l2(0), activation='relu', name = 'layer%d' %idx)\n",
    "    vector = layer(vector)\n",
    "\n",
    "# Final prediction layer\n",
    "#prediction = Lambda(lambda x: K.sigmoid(K.sum(x)), output_shape=(1,))(predict_vector)\n",
    "#prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n",
    "# CHANGED to linear for regression\n",
    "outputs = Dense(1, activation='linear', kernel_initializer='lecun_uniform', name = 'prediction')(vector)\n",
    "\n",
    "model = keras.Model(inputs=input_layer, outputs=outputs, name=model_name)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T06:39:09.199038Z",
     "start_time": "2022-10-11T06:38:39.347445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"preliminar\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "entrada (InputLayer)            [(None, 3579)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1508)         0           entrada[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2071)         0           entrada[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb_u (Dense)                   (None, 8)            12072       lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "emb_i (Dense)                   (None, 8)            16576       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8)            0           emb_u[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8)            0           emb_i[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16)           0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer64 (Dense)                 (None, 64)           1088        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer32 (Dense)                 (None, 32)           2080        layer64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer16 (Dense)                 (None, 16)           528         layer32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer8 (Dense)                  (None, 8)            136         layer16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Dense)              (None, 1)            9           layer8[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 32,489\n",
      "Trainable params: 32,489\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "460/460 [==============================] - 1s 3ms/step - loss: 0.8289 - val_loss: 0.6536\n",
      "Epoch 2/25\n",
      "460/460 [==============================] - 1s 3ms/step - loss: 0.5998 - val_loss: 0.6323\n",
      "Epoch 3/25\n",
      "460/460 [==============================] - 1s 3ms/step - loss: 0.5801 - val_loss: 0.6285\n",
      "Epoch 4/25\n",
      "460/460 [==============================] - 1s 3ms/step - loss: 0.5691 - val_loss: 0.6314\n",
      "Epoch 5/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.5617 - val_loss: 0.6287\n",
      "Epoch 6/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.5568 - val_loss: 0.6326\n",
      "Epoch 7/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.5504 - val_loss: 0.6361\n",
      "Epoch 8/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.5433 - val_loss: 0.6355\n",
      "Epoch 9/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.5376 - val_loss: 0.6337\n",
      "Epoch 10/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.5289 - val_loss: 0.6294\n",
      "Epoch 11/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.5219 - val_loss: 0.6337\n",
      "Epoch 12/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.5117 - val_loss: 0.6389\n",
      "Epoch 13/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.5013 - val_loss: 0.6365\n",
      "Epoch 14/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4925 - val_loss: 0.6437\n",
      "Epoch 15/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4829 - val_loss: 0.6363\n",
      "Epoch 16/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4748 - val_loss: 0.6408\n",
      "Epoch 17/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4667 - val_loss: 0.6390\n",
      "Epoch 18/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4579 - val_loss: 0.6448\n",
      "Epoch 19/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4516 - val_loss: 0.6462\n",
      "Epoch 20/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4439 - val_loss: 0.6485\n",
      "Epoch 21/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4379 - val_loss: 0.6492\n",
      "Epoch 22/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4320 - val_loss: 0.6500\n",
      "Epoch 23/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4264 - val_loss: 0.6504\n",
      "Epoch 24/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4213 - val_loss: 0.6518\n",
      "Epoch 25/25\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.4158 - val_loss: 0.6560\n"
     ]
    }
   ],
   "source": [
    "BATCH=64\n",
    "EPOCH=25\n",
    "\n",
    "num_users = dataset.get_num_users()\n",
    "num_items = dataset.get_num_items()\n",
    "(x_train, x_val, y_train, y_val) = dataset.get_train_val()\n",
    "train_secuencer = OneHotGenerator(x_train, y_train, num_users, num_items, BATCH)\n",
    "val_secuencer = OneHotGenerator(x_val, y_val, num_users, num_items, BATCH)\n",
    "\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001)\n",
    "    #optimizer=keras.optimizers.Nadam()\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_secuencer,\n",
    "    validation_data=val_secuencer,\n",
    "    epochs=EPOCH,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T06:39:09.340657Z",
     "start_time": "2022-10-11T06:39:09.202684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 1ms/step - loss: 0.6592\n"
     ]
    }
   ],
   "source": [
    "(x_test, y_test) = dataset.get_test()\n",
    "test_secuencer = OneHotGenerator(x_test, y_test, num_users, num_items, BATCH)\n",
    "\n",
    "results = model.evaluate(test_secuencer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T06:39:09.351105Z",
     "start_time": "2022-10-11T06:39:09.343448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6592480540275574"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T06:55:54.908260Z",
     "start_time": "2022-10-11T06:55:54.787906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlp_agg\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "entrada (InputLayer)            [(None, 3579)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1508)         0           entrada[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb_u (Dense)                   (None, 8)            12072       lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 8)            0           emb_u[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer64 (Dense)                 (None, 64)           576         flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer32 (Dense)                 (None, 32)           2080        layer64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer16 (Dense)                 (None, 16)           528         layer32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer1508 (Dense)               (None, 1508)         25636       layer16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 2071)         0           entrada[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 3579)         0           layer1508[0][0]                  \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "preliminar (Functional)         (None, 1)            32489       concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 73,381\n",
      "Trainable params: 40,892\n",
      "Non-trainable params: 32,489\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = layers.Input(shape=ds_shape, name=\"entrada\")\n",
    "\n",
    "e_user = layers.Dense(\n",
    "    k,\n",
    "    name=\"emb_u\",\n",
    "    kernel_initializer = 'normal', kernel_regularizer = l2(regs[0])\n",
    ")(layers.Lambda(lambda x: x[:, 0:nuser])(input_layer))\n",
    "\n",
    "user_latent = Flatten()(e_user)\n",
    "\n",
    "vector = user_latent\n",
    "# MLP layers\n",
    "for idx in [64,32,16,nuser]:\n",
    "    layer = Dense(idx, kernel_regularizer= l2(0), activation='relu', name = 'layer%d' %idx)\n",
    "    vector = layer(vector)\n",
    "\n",
    "one_hot_item = layers.Lambda(lambda x: x[:, nuser:])(input_layer)\n",
    "multihot_user = vector\n",
    "mlp_agg = Concatenate()([multihot_user, one_hot_item])\n",
    "model.trainable = False\n",
    "\n",
    "agg_mlp_model = model(mlp_agg)\n",
    "\n",
    "model = keras.Model(inputs=input_layer, outputs=agg_mlp_model, name='mlp_agg')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T06:51:33.249027Z",
     "start_time": "2022-10-11T06:51:33.244374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 3579])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (BASE)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
